{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac13f6c-a061-4c24-a5fc-081ea6283a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab7de8-eb23-47d7-a771-28a2397d0a54",
   "metadata": {},
   "source": [
    "# Modelo CNN con generador, embedings de smiles y data-augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bee0a3-79a7-43b2-9ac2-15cad5630e22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 12:35:27.664839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatagen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m smiles_dict, smiles_to_seq\n",
      "File \u001b[0;32m~/proyectos/itba-deeplearning/bioinformatics_final_project/datagen.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataaug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SmilesEnumerator\n\u001b[1;32m      8\u001b[0m smiles_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m7\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m9\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m11\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m12\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m13\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m14\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m16\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m17\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m11\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     15\u001b[0m     }\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/bioinformatics-final-project-Fa7YnMsJ-py3.9/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/bioinformatics-final-project-Fa7YnMsJ-py3.9/lib/python3.9/site-packages/tensorflow/python/__init__.py:36\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/bioinformatics-final-project-Fa7YnMsJ-py3.9/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py:62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "from datagen import smiles_dict, smiles_to_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa799f-b75c-4d23-869b-fea6d041a6e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### smiles_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9312c-4232-414c-b0ff-1e6fb6dba1a6",
   "metadata": {},
   "source": [
    "smiles_dict nos da un tokenizador para simplificar el problema. Puede ver como se construyó en la notebook **deep_chem**.\n",
    "Si al momento de correr el modelo con este diccionario encuentra problemas de key_error, puede agregar los faltantes al diccionario\n",
    "\n",
    "Mirar dentro de **datagen.py** como se usa este diccionario con la función **smiles_to_seq** para tokenizar. El código es muy sencillo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ef48f-4b67-42a4-83af-e7d0bb980c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(smiles_dict), smiles_dict, sep=\" / \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103a4e0-038d-406b-aab8-5c9452ede04e",
   "metadata": {},
   "source": [
    "# Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a2d75-f0f4-4563-9842-c68d77118161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/acetylcholinesterase_02_bioactivity_data_preprocessed.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fcdcb-ad53-4ebf-bc17-a241c79269da",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_idx = df['canonical_smiles'].apply(len).argmax()\n",
    "min_len_idx = df['canonical_smiles'].apply(len).argmin()\n",
    "max_sequence_len = len(df['canonical_smiles'].iloc[max_len_idx]) + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1554fa5-d9ee-456c-be20-80571e319fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9675f7e8-a8e6-44ff-b1c5-b229cbbeb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['canonical_smiles'].values\n",
    "y = df['pIC50'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87900d-dca2-4f0e-a5c2-58fef4266a67",
   "metadata": {},
   "source": [
    "# Data augmentation:\n",
    "\n",
    "https://arxiv.org/pdf/1703.07076.pdf\n",
    "\n",
    "https://github.com/EBjerrum/molvecgen\n",
    "\n",
    "https://github.com/Ebjerrum/SMILES-enumeration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605fb205-e7e7-45fb-99bd-2e6f490977a8",
   "metadata": {},
   "source": [
    "En la publicación de arriba se describe una técnica de aumentación de datos para los smiles. Leerla si es de su interes (Opcional)\n",
    "\n",
    "En el módulo **dataug.py**, tomando como referencia los repositorios arriba citados se implementó la aumentación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4733af9-9c49-4c2f-bb1d-dc7f180cc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataaug import SmilesEnumerator\n",
    "\n",
    "sme = SmilesEnumerator()\n",
    "\n",
    "for i in range(10):\n",
    "    print(sme.randomize_smiles('CCOc1nn(-c2cccc(OCc3ccccc3)c2)c(=O)o1'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1923d3-1df0-4df1-a43c-ec30eff6fe4d",
   "metadata": {},
   "source": [
    "# DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a3c67-9f44-4052-9e93-9e274f08c1b5",
   "metadata": {},
   "source": [
    "Construir un generador al que se le pase al instanciarlo:\n",
    "- X: smiles (formula química)\n",
    "- y: pIC50\n",
    "- batch_size\n",
    "- max_sequence_len (int): La máxima longitud de las secuencias (para hacer el padding)\n",
    "- data_augmentation (boolean): si quiero hacer o no data-augmentation. \n",
    "- shuffle (boolean)\n",
    "\n",
    "Guardarlo en el módulo **datagen.py** con el nombre de la clase **DataGenerator**\n",
    "\n",
    "Notar que el módulo **datagen.py** ya tiene una estructura para completar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602f3e4-24aa-41ee-9f83-89ec3acc96f1",
   "metadata": {},
   "source": [
    "### Importamos el módulo y lo probamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bce05-2685-4c84-8a43-c3dd8ed74e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa520a-a84b-4d8b-a22e-6b4a5cc7e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgen = DataGenerator(X, y, max_sequence_len, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3212e31-dd2c-4ebc-b9a0-78ed4bb5d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dgen) * dgen.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133033ed-a3b1-4572-aaaf-f4cb2a055724",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X_b, y_b) in enumerate(dgen):\n",
    "    print(f'{i}\\r', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233710d-9274-4dae-b22d-cf42b57ad862",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a95e6-2486-42fb-97fb-865e3fd6a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9840f-2f41-45be-99a5-f05f3ab73ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=88\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70091a76-b1b2-44b8-a63f-7b62f55069c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=88\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad17134-3e15-42d1-b8c9-0c793169e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929de60c-7128-4e35-8efc-4b5f9ee9471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72855a-8310-4a5e-b5c8-7fe181ea8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgen_train = DataGenerator(X_train, y_train, seq_length=max_sequence_len, batch_size=128, data_augmentation=True)\n",
    "dgen_val = DataGenerator(X_val, y_val, seq_length=max_sequence_len, batch_size=128, data_augmentation=False)\n",
    "dgen_test = DataGenerator(X_test, y_test, seq_length=max_sequence_len, batch_size=128, data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf28e8f-b29e-4b60-b7a2-4f08ff5a547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X_b, y_b) in enumerate(dgen_test):\n",
    "    print(f'{i}\\r', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c7f1c-7f3e-40b4-9fb5-5b3ea340c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85941430-2534-4d7d-ba52-d934a31c1bf6",
   "metadata": {},
   "source": [
    "# Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5d1bf-4640-4cbc-acd3-72881ddcc07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Activation, Embedding, Flatten, Dropout, \n",
    "    Concatenate, Input,\n",
    "    Conv1D, MaxPool1D, GlobalAveragePooling1D\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308048c-be62-473c-965a-4a11d0b613d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica\n",
    "def R2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def calculate_r2(y_true, y_pred, _round=8):\n",
    "    return round(1 - ((y_true - y_pred.reshape(-1)) ** 2).sum() / ((y_true - y_true.mean()) ** 2).sum(), _round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362dcc3-1e89-4d8c-b5e6-2b67a5d37074",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp = ModelCheckpoint(\n",
    "    'models/best_model_{epoch}', \n",
    "    save_best_only=True, \n",
    "    save_format=\"h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03bbe2-e1ac-481d-8f17-a390c31242b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,                  # number of epochs with no improvement (0 means the training \n",
    "                                  # is terminated as soon as the performance measure gets worse \n",
    "                                  # from one epoch to the next)\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff491079-3a55-4902-b5c0-52efe549d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=\"logs/fit/model-default\",\n",
    "    histogram_freq=1,\n",
    "    write_graph=False,\n",
    "    write_images=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa4201-cd4e-4d7f-b695-9e32b5dfe1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar modelo de TextCNN\n",
    "\n",
    "def text_cnn_1d(sequence_length, vocab_size, embedding_size, filter_sizes, num_filters):\n",
    "    \"\"\" \"\"\"\n",
    "    \n",
    "    # ref: https://analyticsindiamag.com/guide-to-text-classification-using-textcnn/\n",
    "\n",
    "    max_pool_div = 4  # new\n",
    "    input_x = Input(shape=(sequence_length,), name='input_x')\n",
    "    embedding = Embedding(vocab_size + 1, embedding_size, name='embedding')(input_x)\n",
    "    pooled_outputs = []\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "    \n",
    "        conv = Conv1D(\n",
    "            filters=num_filters, \n",
    "            kernel_size=filter_size, \n",
    "            activation=\"relu\"\n",
    "        )(embedding)\n",
    "\n",
    "        max_p = MaxPool1D(\n",
    "            pool_size=(sequence_length - filter_size + 1) // max_pool_div\n",
    "        )(conv)\n",
    "\n",
    "        pooled_outputs.append(max_p)    \n",
    "\n",
    "    h_pool = Concatenate(axis=2)(pooled_outputs)\n",
    "    dense = Flatten()(h_pool)\n",
    "    dense = Dense(100, activation=\"relu\")(dense)\n",
    "    dense = Dense(50, activation=\"relu\")(dense)\n",
    "    dense = Dense(1)(dense) # Salida\n",
    "\n",
    "    model = Model(input_x, dense)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db729c-1a80-40ad-b3e5-59329b172ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámtros de referencia\n",
    "FILTER_SIZES = (3, 4, 5)\n",
    "NUM_FILTERS = 128\n",
    "vocab_size = len(smiles_dict)\n",
    "embeddings_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0903a5-9840-4994-9964-8e2d3e9eb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = text_cnn_1d(\n",
    "    max_sequence_len,\n",
    "    vocab_size,\n",
    "    embeddings_size,\n",
    "    FILTER_SIZES,\n",
    "    NUM_FILTERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138b411-fc5d-41bd-8c8e-c12580cc5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=[R2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1616d0-c1e3-43e0-b779-e415b94b7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorboard\n",
    "from tensorboard import notebook\n",
    "notebook.list() \n",
    "\n",
    "# %tensorboard --logdir logs/fit/\n",
    "# !tensorboard --logdir logs/fit/ -> Run in your cli (with poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37ab4e-0e3a-44c6-a572-ba5c67930c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"modelTEextCNN\"\n",
    "tensorboard.log_dir = f\"logs/fit/model-{model_name}-{dt.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "EPOCHS = 500  # La idea es que se detenga por el earlyStopping\n",
    "\n",
    "history = model.fit(\n",
    "    dgen_train, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=dgen_val,\n",
    "    callbacks=[\n",
    "        # mcp,\n",
    "        early_stopping,\n",
    "        tensorboard\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c6dbc-6e07-47b7-a24c-a07c6e0ecaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "\n",
    "X_test_eval = []\n",
    "y_t_eval = []\n",
    "\n",
    "for X_t, y_t in dgen_test:\n",
    "    X_test_eval = X_test_eval + [list(t) for t in X_t]\n",
    "    y_t_eval = y_t_eval + list(y_t)\n",
    "X_test_eval = np.array(X_test_eval)\n",
    "y_test = np.array(y_t_eval)\n",
    "\n",
    "X_test_eval.shape, y_test.shape\n",
    "y_pred = model.predict(X_test_eval)\n",
    "\n",
    "print(f\"R2: {calculate_r2(y_test, y_pred, 3)}; ref value: 0.498\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a6260-e31f-4e5e-a923-ce67bf31152f",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9d7e6-2789-459e-b9dd-593e10847fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "ax = sns.regplot(x=y_test, y=y_pred, scatter_kws={'alpha':0.4})\n",
    "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.figure.set_size_inches(5, 5)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f2cc6-8687-4676-b1aa-437914379f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics_final_project",
   "language": "python",
   "name": "bioinformatics_final_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
